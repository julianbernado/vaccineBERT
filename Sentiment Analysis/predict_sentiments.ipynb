{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (conda)"
  },
  "interpreter": {
   "hash": "4138dce0fa74a6453ad2e7bd72e4222e8a2660a917e8bc73b32b3d62a6a94433"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Predicting Sentiment from Tweets using BERT\n",
    "\n",
    "The following file will take in a dataset containing Tweets, do some pre-processing on the Tweets, then use an Ensemble BERT model to predict the sentiments of Tweets. Just follow the instructions below and run each cell subsequently."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Installing libraries\n",
    "\n",
    "To run this code natively on your own computers instead of on Docker, you'll have to install the following libraries. \n",
    "\n",
    "* `pip install nltk`\n",
    "* `pip3 install emoji`\n",
    "* `pip install pandas`\n",
    "* `pip install transformers`\n",
    "* `pip install datasets`\n",
    "* `pip3 install torch torchvision`\n",
    "* `pip install -U scikit-learn`\n",
    "* `pip install regex`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Pre-processing\n",
    "\n",
    "First, we'll pre-process your data to remove any URLs, emojis, or username mentions. Important to note before running this code is that you should have your dataset of Tweets in the same directory as this notebook, and you should replace the placeholder names in the code below. `\"TWEETSETNAME.csv\"` should be replaced by the name of your dataset, and `\"TWEETSCOLUMN\"` should be replaced by the name of the column in your CSV file that has the Tweets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"../TWEETSETNAME.csv\") # Change TWEETSETNAME to the name of your CSV file\n",
    "tweets['label'] = int(1)\n",
    "from TweetNormalizer import normalizeTweet\n",
    "tweets_column = \"TWEETSCOLUMN\" # Change TWEETSCOLUMN to the name of the column in your CSV with the text of the Tweets\n",
    "tweets[tweets_column] = tweets[tweets_column].apply(normalizeTweet)\n",
    "tweets.to_csv(\"tweets_to_predict.csv\", index = False)\n",
    "tweets[0:5].to_csv(\"tweets_to_predict_test.csv\", index = False)"
   ]
  },
  {
   "source": [
    "## Test Run\n",
    "\n",
    "Running the full algorithm on all of your Tweets may take a while, so to make sure that everything is set up correctly run the cell below. If you get the first few rows of a dataframe at the end of the following output, you should be good!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 5\n",
      "  Batch size = 4\n",
      "100%|██████████| 2/2 [00:00<00:00, 17.37it/s]Didn't find file ../model/twitter-roberta_SA\\added_tokens.json. We won't load it.\n",
      "Didn't find file ../model/twitter-roberta_SA\\tokenizer.json. We won't load it.\n",
      "loading file ../model/twitter-roberta_SA\\vocab.json\n",
      "loading file ../model/twitter-roberta_SA\\merges.txt\n",
      "loading file None\n",
      "loading file ../model/twitter-roberta_SA\\special_tokens_map.json\n",
      "loading file ../model/twitter-roberta_SA\\tokenizer_config.json\n",
      "loading file None\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.84it/s]\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file ../model/twitter-roberta_SA\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../model/twitter-roberta_SA\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../model/twitter-roberta_SA.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5\n",
      "  Batch size = 4\n",
      "100%|██████████| 2/2 [00:00<00:00, 16.85it/s]Didn't find file ../model/bertweet-covid19-base-uncased_SA\\tokenizer.json. We won't load it.\n",
      "loading file ../model/bertweet-covid19-base-uncased_SA\\vocab.txt\n",
      "loading file ../model/bertweet-covid19-base-uncased_SA\\bpe.codes\n",
      "loading file ../model/bertweet-covid19-base-uncased_SA\\added_tokens.json\n",
      "loading file ../model/bertweet-covid19-base-uncased_SA\\special_tokens_map.json\n",
      "loading file ../model/bertweet-covid19-base-uncased_SA\\tokenizer_config.json\n",
      "loading file None\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.91it/s]\n",
      "Adding <mask> to the vocabulary\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file ../model/bertweet-covid19-base-uncased_SA\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-covid19-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file ../model/bertweet-covid19-base-uncased_SA\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../model/bertweet-covid19-base-uncased_SA.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5\n",
      "  Batch size = 4\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.18it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                          tweet_text  label\n",
       "0  Weve heard that false information about the CO...      2\n",
       "1  Completely agree . Be smart and get the vaccin...      0\n",
       "2  You know whats even crazier than indiscriminat...      2\n",
       "3  Not my problem . Ive not been seeing anyone si...      0\n",
       "4  If those wanting a vaccine got the vaccine why...      2"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Weve heard that false information about the CO...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Completely agree . Be smart and get the vaccin...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>You know whats even crazier than indiscriminat...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Not my problem . Ive not been seeing anyone si...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>If those wanting a vaccine got the vaccine why...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "test_tweets = pd.read_csv(\"tweets_to_predict_test.csv\")\n",
    "test_dataset = Dataset.from_pandas(test_tweets)\n",
    "\n",
    "from sentiment_analysis import get_sentiment_predictions\n",
    "\n",
    "test_tweets[\"label\"] = get_sentiment_predictions(test_dataset[tweets_column])\n",
    "test_tweets[[tweets_column, \"label\"]].head()"
   ]
  },
  {
   "source": [
    "## Actual Run\n",
    "\n",
    "If that displayed correctly, then run the following chunk! Your output should be saved as `tweets_predicted.csv`. It might take a while."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "e ../model/vaccineBert_SA\\bpe.codes\n",
      "loading file ../model/vaccineBert_SA\\added_tokens.json\n",
      "loading file ../model/vaccineBert_SA\\special_tokens_map.json\n",
      "loading file ../model/vaccineBert_SA\\tokenizer_config.json\n",
      "loading file None\n",
      "Adding <mask> to the vocabulary\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file ../model/vaccineBert_SA\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vaccineBert\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file ../model/vaccineBert_SA\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../model/vaccineBert_SA.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "100%|██████████| 250/250 [01:51<00:00,  2.36it/s]Didn't find file ../model/twitter-roberta_SA\\added_tokens.json. We won't load it.\n",
      "Didn't find file ../model/twitter-roberta_SA\\tokenizer.json. We won't load it.\n",
      "loading file ../model/twitter-roberta_SA\\vocab.json\n",
      "loading file ../model/twitter-roberta_SA\\merges.txt\n",
      "loading file None\n",
      "loading file ../model/twitter-roberta_SA\\special_tokens_map.json\n",
      "loading file ../model/twitter-roberta_SA\\tokenizer_config.json\n",
      "loading file None\n",
      "100%|██████████| 250/250 [01:51<00:00,  2.24it/s]\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file ../model/twitter-roberta_SA\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../model/twitter-roberta_SA\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../model/twitter-roberta_SA.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "100%|██████████| 250/250 [02:08<00:00,  1.98it/s]Didn't find file ../model/bertweet-covid19-base-uncased_SA\\tokenizer.json. We won't load it.\n",
      "loading file ../model/bertweet-covid19-base-uncased_SA\\vocab.txt\n",
      "loading file ../model/bertweet-covid19-base-uncased_SA\\bpe.codes\n",
      "loading file ../model/bertweet-covid19-base-uncased_SA\\added_tokens.json\n",
      "loading file ../model/bertweet-covid19-base-uncased_SA\\special_tokens_map.json\n",
      "loading file ../model/bertweet-covid19-base-uncased_SA\\tokenizer_config.json\n",
      "loading file None\n",
      "Adding <mask> to the vocabulary\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file ../model/bertweet-covid19-base-uncased_SA\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-covid19-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file ../model/bertweet-covid19-base-uncased_SA\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../model/bertweet-covid19-base-uncased_SA.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 2/250 [00:00<00:52,  4.75it/s]\u001b[A\n",
      "  1%|          | 3/250 [00:00<01:16,  3.21it/s]\u001b[A\n",
      "  2%|▏         | 4/250 [00:01<01:30,  2.71it/s]\u001b[A\n",
      "  2%|▏         | 5/250 [00:01<01:44,  2.35it/s]\u001b[A\n",
      "  2%|▏         | 6/250 [00:02<01:47,  2.27it/s]\u001b[A\n",
      "  3%|▎         | 7/250 [00:02<01:48,  2.23it/s]\u001b[A\n",
      "  3%|▎         | 8/250 [00:03<01:50,  2.20it/s]\u001b[A\n",
      "  4%|▎         | 9/250 [00:03<01:51,  2.16it/s]\u001b[A\n",
      "  4%|▍         | 10/250 [00:04<01:53,  2.11it/s]\u001b[A\n",
      "  4%|▍         | 11/250 [00:04<01:49,  2.18it/s]\u001b[A\n",
      "  5%|▍         | 12/250 [00:05<01:51,  2.14it/s]\u001b[A\n",
      "  5%|▌         | 13/250 [00:05<01:53,  2.09it/s]\u001b[A\n",
      "  6%|▌         | 14/250 [00:06<01:52,  2.10it/s]\u001b[A\n",
      "  6%|▌         | 15/250 [00:06<01:48,  2.16it/s]\u001b[A\n",
      "  6%|▋         | 16/250 [00:07<01:47,  2.17it/s]\u001b[A\n",
      "  7%|▋         | 17/250 [00:07<01:49,  2.13it/s]\u001b[A\n",
      "  7%|▋         | 18/250 [00:08<01:52,  2.06it/s]\u001b[A\n",
      "  8%|▊         | 19/250 [00:08<01:50,  2.09it/s]\u001b[A\n",
      "  8%|▊         | 20/250 [00:08<01:50,  2.08it/s]\u001b[A\n",
      "  8%|▊         | 21/250 [00:09<01:53,  2.01it/s]\u001b[A\n",
      "  9%|▉         | 22/250 [00:09<01:50,  2.06it/s]\u001b[A\n",
      "  9%|▉         | 23/250 [00:10<01:46,  2.13it/s]\u001b[A\n",
      " 10%|▉         | 24/250 [00:10<01:50,  2.04it/s]\u001b[A\n",
      " 10%|█         | 25/250 [00:11<01:50,  2.05it/s]\u001b[A\n",
      " 10%|█         | 26/250 [00:11<01:50,  2.02it/s]\u001b[A\n",
      " 11%|█         | 27/250 [00:12<01:52,  1.99it/s]\u001b[A\n",
      " 11%|█         | 28/250 [00:12<01:52,  1.97it/s]\u001b[A\n",
      " 12%|█▏        | 29/250 [00:13<01:48,  2.03it/s]\u001b[A\n",
      " 12%|█▏        | 30/250 [00:13<01:48,  2.04it/s]\u001b[A\n",
      " 12%|█▏        | 31/250 [00:14<01:44,  2.10it/s]\u001b[A\n",
      " 13%|█▎        | 32/250 [00:14<01:42,  2.14it/s]\u001b[A\n",
      " 13%|█▎        | 33/250 [00:15<01:40,  2.16it/s]\u001b[A\n",
      " 14%|█▎        | 34/250 [00:15<01:45,  2.06it/s]\u001b[A\n",
      " 14%|█▍        | 35/250 [00:16<01:43,  2.07it/s]\u001b[A\n",
      " 14%|█▍        | 36/250 [00:16<01:41,  2.11it/s]\u001b[A\n",
      " 15%|█▍        | 37/250 [00:17<01:37,  2.19it/s]\u001b[A\n",
      " 15%|█▌        | 38/250 [00:17<01:35,  2.21it/s]\u001b[A\n",
      " 16%|█▌        | 39/250 [00:18<01:38,  2.15it/s]\u001b[A\n",
      " 16%|█▌        | 40/250 [00:18<01:38,  2.14it/s]\u001b[A\n",
      " 16%|█▋        | 41/250 [00:19<01:37,  2.14it/s]\u001b[A\n",
      " 17%|█▋        | 42/250 [00:19<01:37,  2.13it/s]\u001b[A\n",
      " 17%|█▋        | 43/250 [00:19<01:34,  2.18it/s]\u001b[A\n",
      " 18%|█▊        | 44/250 [00:20<01:39,  2.08it/s]\u001b[A\n",
      " 18%|█▊        | 45/250 [00:20<01:39,  2.06it/s]\u001b[A\n",
      " 18%|█▊        | 46/250 [00:21<01:41,  2.02it/s]\u001b[A\n",
      " 19%|█▉        | 47/250 [00:21<01:41,  2.01it/s]\u001b[A\n",
      " 19%|█▉        | 48/250 [00:22<01:41,  1.99it/s]\u001b[A\n",
      " 20%|█▉        | 49/250 [00:23<01:40,  1.99it/s]\u001b[A\n",
      " 20%|██        | 50/250 [00:23<01:41,  1.97it/s]\u001b[A\n",
      " 20%|██        | 51/250 [00:23<01:36,  2.05it/s]\u001b[A\n",
      " 21%|██        | 52/250 [00:24<01:37,  2.03it/s]\u001b[A\n",
      " 21%|██        | 53/250 [00:24<01:38,  2.01it/s]\u001b[A\n",
      " 22%|██▏       | 54/250 [00:25<01:36,  2.03it/s]\u001b[A\n",
      " 22%|██▏       | 55/250 [00:25<01:37,  2.00it/s]\u001b[A\n",
      " 22%|██▏       | 56/250 [00:26<01:35,  2.03it/s]\u001b[A\n",
      " 23%|██▎       | 57/250 [00:26<01:37,  1.98it/s]\u001b[A\n",
      " 23%|██▎       | 58/250 [00:27<01:41,  1.89it/s]\u001b[A\n",
      " 24%|██▎       | 59/250 [00:28<01:37,  1.95it/s]\u001b[A\n",
      " 24%|██▍       | 60/250 [00:28<01:38,  1.93it/s]\u001b[A\n",
      " 24%|██▍       | 61/250 [00:29<01:36,  1.96it/s]\u001b[A\n",
      " 25%|██▍       | 62/250 [00:29<01:32,  2.04it/s]\u001b[A\n",
      " 25%|██▌       | 63/250 [00:30<01:35,  1.96it/s]\u001b[A\n",
      " 26%|██▌       | 64/250 [00:30<01:35,  1.95it/s]\u001b[A\n",
      " 26%|██▌       | 65/250 [00:31<01:35,  1.94it/s]\u001b[A\n",
      " 26%|██▋       | 66/250 [00:31<01:33,  1.98it/s]\u001b[A\n",
      " 27%|██▋       | 67/250 [00:32<01:29,  2.05it/s]\u001b[A\n",
      " 27%|██▋       | 68/250 [00:32<01:28,  2.05it/s]\u001b[A\n",
      " 28%|██▊       | 69/250 [00:33<01:27,  2.06it/s]\u001b[A\n",
      " 28%|██▊       | 70/250 [00:33<01:26,  2.08it/s]\u001b[A\n",
      " 28%|██▊       | 71/250 [00:33<01:23,  2.14it/s]\u001b[A\n",
      " 29%|██▉       | 72/250 [00:34<01:19,  2.25it/s]\u001b[A\n",
      " 29%|██▉       | 73/250 [00:34<01:19,  2.22it/s]\u001b[A\n",
      " 30%|██▉       | 74/250 [00:35<01:21,  2.16it/s]\u001b[A\n",
      " 30%|███       | 75/250 [00:35<01:22,  2.12it/s]\u001b[A\n",
      " 30%|███       | 76/250 [00:36<01:20,  2.17it/s]\u001b[A\n",
      " 31%|███       | 77/250 [00:36<01:19,  2.17it/s]\u001b[A\n",
      " 31%|███       | 78/250 [00:37<01:17,  2.22it/s]\u001b[A\n",
      " 32%|███▏      | 79/250 [00:37<01:18,  2.16it/s]\u001b[A\n",
      " 32%|███▏      | 80/250 [00:38<01:18,  2.16it/s]\u001b[A\n",
      " 32%|███▏      | 81/250 [00:38<01:20,  2.11it/s]\u001b[A\n",
      " 33%|███▎      | 82/250 [00:38<01:17,  2.16it/s]\u001b[A\n",
      " 33%|███▎      | 83/250 [00:39<01:15,  2.21it/s]\u001b[A\n",
      " 34%|███▎      | 84/250 [00:39<01:14,  2.23it/s]\u001b[A\n",
      " 34%|███▍      | 85/250 [00:40<01:13,  2.24it/s]\u001b[A\n",
      " 34%|███▍      | 86/250 [00:40<01:10,  2.31it/s]\u001b[A\n",
      " 35%|███▍      | 87/250 [00:41<01:12,  2.25it/s]\u001b[A\n",
      " 35%|███▌      | 88/250 [00:41<01:12,  2.24it/s]\u001b[A\n",
      " 36%|███▌      | 89/250 [00:42<01:10,  2.28it/s]\u001b[A\n",
      " 36%|███▌      | 90/250 [00:42<01:10,  2.28it/s]\u001b[A\n",
      " 36%|███▋      | 91/250 [00:42<01:11,  2.24it/s]\u001b[A\n",
      " 37%|███▋      | 92/250 [00:43<01:12,  2.18it/s]\u001b[A\n",
      " 37%|███▋      | 93/250 [00:43<01:10,  2.22it/s]\u001b[A\n",
      " 38%|███▊      | 94/250 [00:44<01:10,  2.22it/s]\u001b[A\n",
      " 38%|███▊      | 95/250 [00:44<01:09,  2.23it/s]\u001b[A\n",
      " 38%|███▊      | 96/250 [00:45<01:10,  2.19it/s]\u001b[A\n",
      " 39%|███▉      | 97/250 [00:45<01:08,  2.24it/s]\u001b[A\n",
      " 39%|███▉      | 98/250 [00:46<01:06,  2.28it/s]\u001b[A\n",
      " 40%|███▉      | 99/250 [00:46<01:05,  2.32it/s]\u001b[A\n",
      " 40%|████      | 100/250 [00:46<01:06,  2.25it/s]\u001b[A\n",
      " 40%|████      | 101/250 [00:47<01:07,  2.20it/s]\u001b[A\n",
      " 41%|████      | 102/250 [00:47<01:06,  2.22it/s]\u001b[A\n",
      " 41%|████      | 103/250 [00:48<01:09,  2.12it/s]\u001b[A\n",
      " 42%|████▏     | 104/250 [00:48<01:06,  2.18it/s]\u001b[A\n",
      " 42%|████▏     | 105/250 [00:49<01:06,  2.19it/s]\u001b[A\n",
      " 42%|████▏     | 106/250 [00:49<01:03,  2.25it/s]\u001b[A\n",
      " 43%|████▎     | 107/250 [00:50<01:04,  2.21it/s]\u001b[A\n",
      " 43%|████▎     | 108/250 [00:50<01:04,  2.20it/s]\u001b[A\n",
      " 44%|████▎     | 109/250 [00:51<01:03,  2.22it/s]\u001b[A\n",
      " 44%|████▍     | 110/250 [00:51<01:03,  2.21it/s]\u001b[A\n",
      " 44%|████▍     | 111/250 [00:51<01:02,  2.21it/s]\u001b[A\n",
      " 45%|████▍     | 112/250 [00:52<01:03,  2.17it/s]\u001b[A\n",
      " 45%|████▌     | 113/250 [00:52<01:02,  2.18it/s]\u001b[A\n",
      " 46%|████▌     | 114/250 [00:53<01:03,  2.13it/s]\u001b[A\n",
      " 46%|████▌     | 115/250 [00:53<01:02,  2.17it/s]\u001b[A\n",
      " 46%|████▋     | 116/250 [00:54<01:02,  2.14it/s]\u001b[A\n",
      " 47%|████▋     | 117/250 [00:54<01:01,  2.15it/s]\u001b[A\n",
      " 47%|████▋     | 118/250 [00:55<01:01,  2.14it/s]\u001b[A\n",
      " 48%|████▊     | 119/250 [00:55<00:59,  2.20it/s]\u001b[A\n",
      " 48%|████▊     | 120/250 [00:56<00:58,  2.22it/s]\u001b[A\n",
      " 48%|████▊     | 121/250 [00:56<00:58,  2.21it/s]\u001b[A\n",
      " 49%|████▉     | 122/250 [00:57<00:58,  2.17it/s]\u001b[A\n",
      " 49%|████▉     | 123/250 [00:57<00:58,  2.18it/s]\u001b[A\n",
      " 50%|████▉     | 124/250 [00:57<00:56,  2.21it/s]\u001b[A\n",
      " 50%|█████     | 125/250 [00:58<00:56,  2.22it/s]\u001b[A\n",
      " 50%|█████     | 126/250 [00:58<00:56,  2.21it/s]\u001b[A\n",
      " 51%|█████     | 127/250 [00:59<00:57,  2.16it/s]\u001b[A\n",
      " 51%|█████     | 128/250 [00:59<00:55,  2.18it/s]\u001b[A\n",
      " 52%|█████▏    | 129/250 [01:00<00:54,  2.20it/s]\u001b[A\n",
      " 52%|█████▏    | 130/250 [01:00<00:52,  2.28it/s]\u001b[A\n",
      " 52%|█████▏    | 131/250 [01:01<00:54,  2.19it/s]\u001b[A\n",
      " 53%|█████▎    | 132/250 [01:01<00:54,  2.18it/s]\u001b[A\n",
      " 53%|█████▎    | 133/250 [01:02<00:53,  2.18it/s]\u001b[A\n",
      " 54%|█████▎    | 134/250 [01:02<00:52,  2.21it/s]\u001b[A\n",
      " 54%|█████▍    | 135/250 [01:02<00:52,  2.17it/s]\u001b[A\n",
      " 54%|█████▍    | 136/250 [01:03<00:52,  2.17it/s]\u001b[A\n",
      " 55%|█████▍    | 137/250 [01:03<00:50,  2.24it/s]\u001b[A\n",
      " 55%|█████▌    | 138/250 [01:04<00:51,  2.18it/s]\u001b[A\n",
      " 56%|█████▌    | 139/250 [01:04<00:52,  2.13it/s]\u001b[A\n",
      " 56%|█████▌    | 140/250 [01:05<00:50,  2.17it/s]\u001b[A\n",
      " 56%|█████▋    | 141/250 [01:05<00:50,  2.16it/s]\u001b[A\n",
      " 57%|█████▋    | 142/250 [01:06<00:50,  2.14it/s]\u001b[A\n",
      " 57%|█████▋    | 143/250 [01:06<00:50,  2.12it/s]\u001b[A\n",
      " 58%|█████▊    | 144/250 [01:07<00:50,  2.11it/s]\u001b[A\n",
      " 58%|█████▊    | 145/250 [01:07<00:49,  2.13it/s]\u001b[A\n",
      " 58%|█████▊    | 146/250 [01:08<00:49,  2.12it/s]\u001b[A\n",
      " 59%|█████▉    | 147/250 [01:08<00:48,  2.13it/s]\u001b[A\n",
      " 59%|█████▉    | 148/250 [01:08<00:46,  2.19it/s]\u001b[A\n",
      " 60%|█████▉    | 149/250 [01:09<00:46,  2.16it/s]\u001b[A\n",
      " 60%|██████    | 150/250 [01:09<00:45,  2.21it/s]\u001b[A\n",
      " 60%|██████    | 151/250 [01:10<00:44,  2.24it/s]\u001b[A\n",
      " 61%|██████    | 152/250 [01:10<00:42,  2.32it/s]\u001b[A\n",
      " 61%|██████    | 153/250 [01:11<00:41,  2.36it/s]\u001b[A\n",
      " 62%|██████▏   | 154/250 [01:11<00:42,  2.24it/s]\u001b[A\n",
      " 62%|██████▏   | 155/250 [01:12<00:42,  2.22it/s]\u001b[A\n",
      " 62%|██████▏   | 156/250 [01:12<00:41,  2.26it/s]\u001b[A\n",
      " 63%|██████▎   | 157/250 [01:13<00:43,  2.14it/s]\u001b[A\n",
      " 63%|██████▎   | 158/250 [01:13<00:44,  2.06it/s]\u001b[A\n",
      " 64%|██████▎   | 159/250 [01:14<00:44,  2.03it/s]\u001b[A\n",
      " 64%|██████▍   | 160/250 [01:14<00:44,  2.04it/s]\u001b[A\n",
      " 64%|██████▍   | 161/250 [01:15<00:43,  2.03it/s]\u001b[A\n",
      " 65%|██████▍   | 162/250 [01:15<00:44,  1.99it/s]\u001b[A\n",
      " 65%|██████▌   | 163/250 [01:16<00:43,  2.00it/s]\u001b[A\n",
      " 66%|██████▌   | 164/250 [01:16<00:41,  2.08it/s]\u001b[A\n",
      " 66%|██████▌   | 165/250 [01:16<00:39,  2.16it/s]\u001b[A\n",
      " 66%|██████▋   | 166/250 [01:17<00:39,  2.12it/s]\u001b[A\n",
      " 67%|██████▋   | 167/250 [01:17<00:39,  2.10it/s]\u001b[A\n",
      " 67%|██████▋   | 168/250 [01:18<00:39,  2.09it/s]\u001b[A\n",
      " 68%|██████▊   | 169/250 [01:18<00:38,  2.09it/s]\u001b[A\n",
      " 68%|██████▊   | 170/250 [01:19<00:37,  2.11it/s]\u001b[A\n",
      " 68%|██████▊   | 171/250 [01:19<00:37,  2.13it/s]\u001b[A\n",
      " 69%|██████▉   | 172/250 [01:20<00:36,  2.11it/s]\u001b[A\n",
      " 69%|██████▉   | 173/250 [01:20<00:37,  2.04it/s]\u001b[A\n",
      " 70%|██████▉   | 174/250 [01:21<00:37,  2.04it/s]\u001b[A\n",
      " 70%|███████   | 175/250 [01:21<00:36,  2.05it/s]\u001b[A\n",
      " 70%|███████   | 176/250 [01:22<00:35,  2.11it/s]\u001b[A\n",
      " 71%|███████   | 177/250 [01:22<00:34,  2.13it/s]\u001b[A\n",
      " 71%|███████   | 178/250 [01:23<00:32,  2.22it/s]\u001b[A\n",
      " 72%|███████▏  | 179/250 [01:23<00:32,  2.17it/s]\u001b[A\n",
      " 72%|███████▏  | 180/250 [01:24<00:31,  2.22it/s]\u001b[A\n",
      " 72%|███████▏  | 181/250 [01:24<00:31,  2.20it/s]\u001b[A\n",
      " 73%|███████▎  | 182/250 [01:24<00:31,  2.19it/s]\u001b[A\n",
      " 73%|███████▎  | 183/250 [01:25<00:30,  2.18it/s]\u001b[A\n",
      " 74%|███████▎  | 184/250 [01:25<00:30,  2.18it/s]\u001b[A\n",
      " 74%|███████▍  | 185/250 [01:26<00:29,  2.22it/s]\u001b[A\n",
      " 74%|███████▍  | 186/250 [01:26<00:28,  2.25it/s]\u001b[A\n",
      " 75%|███████▍  | 187/250 [01:27<00:28,  2.22it/s]\u001b[A\n",
      " 75%|███████▌  | 188/250 [01:27<00:27,  2.26it/s]\u001b[A\n",
      " 76%|███████▌  | 189/250 [01:28<00:27,  2.21it/s]\u001b[A\n",
      " 76%|███████▌  | 190/250 [01:28<00:27,  2.18it/s]\u001b[A\n",
      " 76%|███████▋  | 191/250 [01:28<00:26,  2.26it/s]\u001b[A\n",
      " 77%|███████▋  | 192/250 [01:29<00:25,  2.23it/s]\u001b[A\n",
      " 77%|███████▋  | 193/250 [01:29<00:24,  2.30it/s]\u001b[A\n",
      " 78%|███████▊  | 194/250 [01:30<00:23,  2.34it/s]\u001b[A\n",
      " 78%|███████▊  | 195/250 [01:30<00:23,  2.35it/s]\u001b[A\n",
      " 78%|███████▊  | 196/250 [01:31<00:23,  2.27it/s]\u001b[A\n",
      " 79%|███████▉  | 197/250 [01:31<00:24,  2.20it/s]\u001b[A\n",
      " 79%|███████▉  | 198/250 [01:32<00:24,  2.16it/s]\u001b[A\n",
      " 80%|███████▉  | 199/250 [01:32<00:23,  2.15it/s]\u001b[A\n",
      " 80%|████████  | 200/250 [01:33<00:23,  2.17it/s]\u001b[A\n",
      " 80%|████████  | 201/250 [01:33<00:22,  2.16it/s]\u001b[A\n",
      " 81%|████████  | 202/250 [01:33<00:21,  2.20it/s]\u001b[A\n",
      " 81%|████████  | 203/250 [01:34<00:21,  2.22it/s]\u001b[A\n",
      " 82%|████████▏ | 204/250 [01:34<00:20,  2.23it/s]\u001b[A\n",
      " 82%|████████▏ | 205/250 [01:35<00:20,  2.15it/s]\u001b[A\n",
      " 82%|████████▏ | 206/250 [01:35<00:19,  2.20it/s]\u001b[A\n",
      " 83%|████████▎ | 207/250 [01:36<00:20,  2.14it/s]\u001b[A\n",
      " 83%|████████▎ | 208/250 [01:36<00:19,  2.10it/s]\u001b[A\n",
      " 84%|████████▎ | 209/250 [01:37<00:19,  2.10it/s]\u001b[A\n",
      " 84%|████████▍ | 210/250 [01:37<00:18,  2.13it/s]\u001b[A\n",
      " 84%|████████▍ | 211/250 [01:38<00:17,  2.21it/s]\u001b[A\n",
      " 85%|████████▍ | 212/250 [01:38<00:17,  2.17it/s]\u001b[A\n",
      " 85%|████████▌ | 213/250 [01:38<00:16,  2.22it/s]\u001b[A\n",
      " 86%|████████▌ | 214/250 [01:39<00:16,  2.24it/s]\u001b[A\n",
      " 86%|████████▌ | 215/250 [01:39<00:15,  2.28it/s]\u001b[A\n",
      " 86%|████████▋ | 216/250 [01:40<00:15,  2.22it/s]\u001b[A\n",
      " 87%|████████▋ | 217/250 [01:40<00:15,  2.16it/s]\u001b[A\n",
      " 87%|████████▋ | 218/250 [01:41<00:14,  2.16it/s]\u001b[A\n",
      " 88%|████████▊ | 219/250 [01:41<00:13,  2.23it/s]\u001b[A\n",
      " 88%|████████▊ | 220/250 [01:42<00:13,  2.28it/s]\u001b[A\n",
      " 88%|████████▊ | 221/250 [01:42<00:12,  2.24it/s]\u001b[A\n",
      " 89%|████████▉ | 222/250 [01:43<00:12,  2.18it/s]\u001b[A\n",
      " 89%|████████▉ | 223/250 [01:43<00:12,  2.21it/s]\u001b[A\n",
      " 90%|████████▉ | 224/250 [01:43<00:11,  2.18it/s]\u001b[A\n",
      " 90%|█████████ | 225/250 [01:44<00:11,  2.17it/s]\u001b[A\n",
      " 90%|█████████ | 226/250 [01:44<00:11,  2.17it/s]\u001b[A\n",
      " 91%|█████████ | 227/250 [01:45<00:10,  2.20it/s]\u001b[A\n",
      " 91%|█████████ | 228/250 [01:45<00:10,  2.18it/s]\u001b[A\n",
      " 92%|█████████▏| 229/250 [01:46<00:09,  2.14it/s]\u001b[A\n",
      " 92%|█████████▏| 230/250 [01:46<00:09,  2.14it/s]\u001b[A\n",
      " 92%|█████████▏| 231/250 [01:47<00:08,  2.17it/s]\u001b[A\n",
      " 93%|█████████▎| 232/250 [01:47<00:07,  2.27it/s]\u001b[A\n",
      " 93%|█████████▎| 233/250 [01:48<00:07,  2.25it/s]\u001b[A\n",
      " 94%|█████████▎| 234/250 [01:48<00:07,  2.22it/s]\u001b[A\n",
      " 94%|█████████▍| 235/250 [01:48<00:06,  2.23it/s]\u001b[A\n",
      " 94%|█████████▍| 236/250 [01:49<00:06,  2.15it/s]\u001b[A\n",
      " 95%|█████████▍| 237/250 [01:49<00:06,  2.15it/s]\u001b[A\n",
      " 95%|█████████▌| 238/250 [01:50<00:05,  2.14it/s]\u001b[A\n",
      " 96%|█████████▌| 239/250 [01:50<00:04,  2.21it/s]\u001b[A\n",
      " 96%|█████████▌| 240/250 [01:51<00:04,  2.19it/s]\u001b[A\n",
      " 96%|█████████▋| 241/250 [01:51<00:04,  2.19it/s]\u001b[A\n",
      " 97%|█████████▋| 242/250 [01:52<00:03,  2.21it/s]\u001b[A\n",
      " 97%|█████████▋| 243/250 [01:52<00:03,  2.23it/s]\u001b[A\n",
      " 98%|█████████▊| 244/250 [01:53<00:02,  2.22it/s]\u001b[A\n",
      " 98%|█████████▊| 245/250 [01:53<00:02,  2.25it/s]\u001b[A\n",
      " 98%|█████████▊| 246/250 [01:53<00:01,  2.27it/s]\u001b[A\n",
      " 99%|█████████▉| 247/250 [01:54<00:01,  2.22it/s]\u001b[A\n",
      " 99%|█████████▉| 248/250 [01:54<00:00,  2.16it/s]\u001b[A\n",
      "100%|█████████▉| 249/250 [01:55<00:00,  2.14it/s]\u001b[A\n",
      "100%|██████████| 250/250 [01:55<00:00,  2.07it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"tweets_to_predict.csv\")\n",
    "dataset = Dataset.from_pandas(tweets)\n",
    "\n",
    "sentiment_integers =  get_sentiment_predictions(dataset[tweets_column])\n",
    "sentiment_dict = {0: \"positive\", 1: \"negative\", 2: \"neutral\"}\n",
    "tweets[\"label\"] = [sentiment_dict[i] for i in sentiment_integers] # Translate integers into actual sentiment labels\n",
    "tweets.to_csv(\"tweets_predicted.csv\", index = False)"
   ]
  }
 ]
}