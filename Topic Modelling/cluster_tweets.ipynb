{
 "cells": [
  {
   "source": [
    "# Topic Modelling via Clustering\n",
    "\n",
    "This Python notebook translates Tweets into vectors using vaccineBERT. Just follow the instructions below!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Installing packages\n",
    "\n",
    "Make sure you've installed the following packages:\n",
    "\n",
    "* pip install pandas\n",
    "* pip install numpy\n",
    "* pip install umap-learn\n",
    "* pip install hdbscan\n",
    "* pip install sentence-transformers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "color-mechanics",
   "metadata": {},
   "source": [
    "## Import BERT Model\n",
    "\n",
    "First we import the vaccineBERT model. This takes a while, so no need to run again once you've done it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import models\n",
    "word_embedding_model = models.Transformer('../model/vaccineBert_SA/', max_seq_length=128)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=False,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=True)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-pleasure",
   "metadata": {},
   "source": [
    "## Get Text Data\n",
    "\n",
    "Now, we get the actual set of Tweets that we'll be clustering. Make sure though to change the name `\"TWEETSETNAME.csv\"` to the name of your CSV, and change `\"TWEETSCOLUMN\"` to the name of the column in your dataset that has Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TweetNormalizer import normalizeTweet\n",
    "\n",
    "dataset = pd.read_csv(\"../TWEETSETNAME.csv\") # Change TWEETSETNAME.csv to the name of your CSV, but keep the ../ before it\n",
    "tweets_column = \"TWEETSCOLUMN\" # Change \"TWEETSCOLUMN\" to the name of the column in your dataset that has tweets\n",
    "text_data = dataset[tweets_column].apply(normalizeTweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-shape",
   "metadata": {},
   "source": [
    "## Embedding Texts \n",
    "\n",
    "Now, we embed these tweets into vectors that we can then cluster. Since this code takes a while to run, this file will check if you've done so already and use the previously saved result if so. If you want to calculate new embeddings for a new dataset, delete the \"embeddings\" folder in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "if os.path.isfile(\"embeddings/embeddings.npy\"):\n",
    "    embeddings = np.load('embeddings/embeddings.npy')\n",
    "else:\n",
    "    embeddings = model.encode(text_data, show_progress_bar=True)\n",
    "    os.mkdir(\"embeddings\")\n",
    "    with open('embeddings/embeddings.npy', 'w+'): pass\n",
    "    np.save('embeddings/embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-northern",
   "metadata": {},
   "source": [
    "## UMAP algorithm\n",
    "\n",
    "From here, we use the UMAP algorithm to project our embeddings into a lower dimensional space. Read the following resources to understand the UMAP algorithm more.\n",
    "\n",
    "* [Understanding UMAP](https://pair-code.github.io/understanding-umap/)\n",
    "* [UMAP Doc](https://umap-learn.readthedocs.io/en/latest/)\n",
    "\n",
    "Also, feel free to play with the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "umap_embeddings = umap.UMAP(n_neighbors=5, \n",
    "                            n_components=20, \n",
    "                            metric='cosine', random_state = 123, min_dist= 0.1,\n",
    "                           ).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-atmosphere",
   "metadata": {},
   "source": [
    "## HDBSCAN algorithm\n",
    "\n",
    "Now, we use the HDBSCAN algorithm to cluster our Tweets. The following link provides some information about the algorithm.\n",
    "\n",
    "* [HDBSCAN Doc](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html)\n",
    "\n",
    "Again, free to play with parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=10, min_samples= 2,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom')\n",
    "\n",
    "cluster = cluster.fit(umap_embeddings)"
   ]
  },
  {
   "source": [
    "The following output will show the labels of the different clusters. -1 is the label of Tweets that were not assigned to a cluster by HDBSCAN. We also save the original dataset with labels for the clusters output by HDBSCAN as the file `clustered_data.csv`. With this CSV, you can examine which Tweets belong to which clusters and try to extract topics."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(cluster.labels_)\n",
    "dataset[\"cluster\"] = cluster.labels_\n",
    "dataset.to_csv(\"clustered_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-wrapping",
   "metadata": {},
   "source": [
    "## 2D Plot - Clustered from HDSBCAN\n",
    "\n",
    "We now project our embeddings into two dimensional space so that they can be plotted. We color the points based on the assigned clusters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce dimension\n",
    "umap_data = umap.UMAP(n_neighbors=10, n_components=2, min_dist=0.1, random_state = 123, metric='cosine').fit_transform(embeddings)\n",
    "\n",
    "# Prepare data\n",
    "result = pd.DataFrame(umap_data[cluster.labels_!= -1], columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_[cluster.labels_!= -1]\n",
    "\n",
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.scatter(result.x, result.y, c=result.labels, s=5, cmap='plasma')\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "4138dce0fa74a6453ad2e7bd72e4222e8a2660a917e8bc73b32b3d62a6a94433"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}